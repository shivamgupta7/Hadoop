{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "from pyspark.sql import SQLContext, Row\n",
    "import pyspark.sql.functions as sqlFun\n",
    "import subprocess\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_files(path):\n",
    "    '''\n",
    "    Show files on given path in HDFS and return all existing paths.\n",
    "    '''\n",
    "    some_path = 'hdfs dfs -ls {0}'.format(path).split()\n",
    "    files = subprocess.check_output(some_path,encoding='utf8').strip().split('\\n')\n",
    "    paths = []\n",
    "    for path in files[1:]:\n",
    "        # print(path)\n",
    "        paths.append(path)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path,header,inferSchema):\n",
    "    '''\n",
    "    Reading file from HDFS.\n",
    "    '''\n",
    "    try:\n",
    "        df = sqlContext.read.csv('hdfs://{0}'.format(path),header = header, inferSchema = inferSchema)\n",
    "        return df\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_idle_hour(detail,users_data):\n",
    "    global count_idle\n",
    "    if detail.keyboard == 0 and detail.mouse == 0:\n",
    "        count_idle += 1\n",
    "    else:\n",
    "        count_idle = 0\n",
    "    if count_idle >= 5:\n",
    "        if count_idle == 5:\n",
    "            users_data[detail.user_name]['idle_time'] = users_data[detail.user_name].get('idle_time') + dt.timedelta(0, 1500)\n",
    "        else:\n",
    "            users_data[detail.user_name]['idle_time'] = users_data[detail.user_name].get('idle_time') + dt.timedelta(0, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_working_hours(details,date,initial_time,users_data):\n",
    "    delta = (dt.datetime.strptime(date + initial_time, '%Y-%m-%d %H:%M:%S') \\\n",
    "             + (details.end_time - details.start_time)) - details.idle_time\n",
    "    users_data[details.user_name]['working_hour'] = dt.datetime.strptime(date + initial_time, '%Y-%m-%d %H:%M:%S') + delta\n",
    "    users_data[details.user_name]['start_time'] = details.start_time\n",
    "    users_data[details.user_name]['end_time'] = details.end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/user/spark-file/input/'\n",
    "users_data = dict()\n",
    "start_timing = ' 08:30:00'\n",
    "end_timing = ' 19:30:00'\n",
    "initial_time = ' 00:00:00'\n",
    "schema = StructType([\n",
    "    StructField(\"user_name\", StringType(),True),\n",
    "    StructField(\"start_time\", TimestampType(),True),\n",
    "    StructField(\"end_time\", TimestampType(),True),\n",
    "    StructField(\"idle_time\", TimestampType(),True),\n",
    "    StructField(\"working_hour\", TimestampType(),True)])\n",
    "total_log = sqlContext.createDataFrame(sc.emptyRDD(), schema)\n",
    "def complete_operation(directory, start_timing, end_timing, initial_time, df_total):\n",
    "    try:\n",
    "        paths = show_files(directory)\n",
    "        users_data = dict()\n",
    "        for path in paths:\n",
    "            path = path.split()[-1]\n",
    "            log = read_file(path,header=True,inferSchema=True)\n",
    "            select_column = ['DateTime', 'user_name', 'keyboard', 'mouse']\n",
    "            log = log.select(*select_column)\n",
    "            date = dt.datetime.strftime(log.collect()[0][0],'%Y-%m-%d')\n",
    "            print('Start ' + date)\n",
    "            log = log[log['DateTime'] >= date + start_timing]\n",
    "            log = log[log['DateTime'] <= date + end_timing]\n",
    "            unique_users = log.select('user_name').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "            start_DateTime = dt.datetime.strptime(date + initial_time, '%Y-%m-%d %H:%M:%S')\n",
    "    #         print(start_DateTime)\n",
    "            for user in unique_users:\n",
    "                users_data[user] = {\n",
    "                'start_time' : start_DateTime,\n",
    "                'end_time' : start_DateTime,\n",
    "                'idle_time': start_DateTime,\n",
    "                'working_hour' : start_DateTime\n",
    "                }\n",
    "            log = log.sort('DateTime')\n",
    "            startTime = log.groupBy(\"user_name\").agg(sqlFun.min(\"DateTime\").alias('start_time'))\n",
    "    #         print(startTime.show(5))\n",
    "            endTime = log.groupBy(\"user_name\").agg(sqlFun.max(\"DateTime\").alias('end_time'))\n",
    "    #         print(endTime.show(5))\n",
    "            for user in unique_users:\n",
    "                count_idle = 0\n",
    "                for row in log.rdd.collect():\n",
    "                    if user == row.user_name:\n",
    "                        cal_idle_hour(row,users_data)\n",
    "    #         print('Done! Idle hour.')\n",
    "            data_rows_idle = [Row(**{'user_name': user, **logs}) for user,logs in users_data.items()]\n",
    "            idleTime = spark.createDataFrame(data_rows_idle).select('user_name', 'idle_time')\n",
    "    #         print(idleTime.show(5))\n",
    "            data_df = idleTime.join(startTime, on=['user_name'], how='inner').join(endTime, on=['user_name'], how='inner')\n",
    "    #         data_df.show(5)\n",
    "            for row in data_df.rdd.collect():\n",
    "                cal_working_hours(row, date, initial_time,users_data)\n",
    "    #         print('Done! Working hour.')\n",
    "            data_as_rows = [Row(**{'user_name': user, **logs}) for user,logs in users_data.items()]\n",
    "            data_final = spark.createDataFrame(data_as_rows).select('user_name', 'start_time', 'end_time', 'idle_time', 'working_hour')\n",
    "    #         data_final.show(5)\n",
    "            print('Done '+ date)\n",
    "            df_total = df_total.unionAll(data_final)\n",
    "            users_data={}\n",
    "        print('Done!')\n",
    "        return total_log\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 hadoopuser supergroup    2547139 2020-05-23 15:29 /user/spark-file/input/CpuLogData2019-10-21.csv\n",
      "-rw-r--r--   1 hadoopuser supergroup    2890670 2020-05-23 15:28 /user/spark-file/input/CpuLogData2019-10-22.csv\n",
      "-rw-r--r--   1 hadoopuser supergroup    3115861 2020-05-23 15:29 /user/spark-file/input/CpuLogData2019-10-23.csv\n",
      "-rw-r--r--   1 hadoopuser supergroup    3956926 2020-05-23 15:29 /user/spark-file/input/CpuLogData2019-10-24.csv\n",
      "Start 2019-10-21\n",
      "Done 2019-10-21\n",
      "Start 2019-10-22\n",
      "Done 2019-10-22\n",
      "Start 2019-10-23\n",
      "Done 2019-10-23\n",
      "Start 2019-10-24\n",
      "Done 2019-10-24\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "usr_log = complete_operation(directory,start_timing,end_timing,initial_time,total_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAtHDFS(df, location):\n",
    "    '''\n",
    "    Save data frame into csv file.\n",
    "    '''\n",
    "    try:\n",
    "        df.repartition(1).write.csv('hdfs://{0}'.format(location), header='true')\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAtMySQL(database,table,userName,paswd):\n",
    "    #Save the dataframe to the table.\n",
    "    try:\n",
    "        data_final.write.format('jdbc').options(\n",
    "              url = 'jdbc:mysql://localhost:3306/' + database,\n",
    "              driver = 'com.mysql.jdbc.Driver',\n",
    "              dbtable = table,\n",
    "              user = userName,\n",
    "              password = paswd).mode('append').save()\n",
    "    except Exception as err:\n",
    "        print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAtHDFS(usr_log, '/user/spark-file/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
